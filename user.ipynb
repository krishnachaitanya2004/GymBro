{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4914f25b",
   "metadata": {},
   "source": [
    "# User Code \n",
    "\n",
    "This notebook takes user video , processes the key frames to compare them to the trainer frames stored in csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c3177",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d16227-931c-4436-adde-2ffca0348577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-10 23:06:40.538524: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-10 23:06:40.542837: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-10 23:06:40.553463: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-10 23:06:40.582870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-10 23:06:40.614496: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-10 23:06:40.628137: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-10 23:06:40.650253: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-10 23:06:42.327126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "pose_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "movenet = pose_model.signatures['serving_default']\n",
    "\n",
    "key_frames_angles = []\n",
    "key_frames_points = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31e1e03-c6ce-489b-888c-78cf8260f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "excersise_name = \"PushUp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2d4a0d-e6db-4a8f-88bc-feb5778878e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(excersise_name,\"key_frame_angles.csv\")\n",
    "\n",
    "with open(csv_file, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = [dict(row) for row in reader]\n",
    "\n",
    "csv_file = os.path.join(excersise_name,\"key_frame_points.csv\")\n",
    "\n",
    "with open(csv_file, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data2 = [dict(row) for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1f2618-f2a8-4126-b252-32b4e3045ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frames_angles = [[float(d[key]) for key in d] for d in data]\n",
    "key_frames_points = [[d[key] for key in d] for d in data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d903b3ff-ab68-4000-8260-12ff07562713",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frames_points = [[[float(co) for co in part.strip(\"[]\").split() ] for part in frame] for frame in key_frames_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82ccc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_frame_count = len(key_frames_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdc9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Returns key points by taking model output on a image\n",
    "def get_keypoints(outputs):\n",
    "    points = outputs['output_0'].numpy()[0, 0, :, :]\n",
    "    nose = points[0]\n",
    "    left_shoulder = points[5]\n",
    "    right_shoulder = points[6]\n",
    "    left_elbow = points[7]\n",
    "    right_elbow = points[8]\n",
    "    left_wrist = points[9]\n",
    "    right_wrist = points[10]\n",
    "    left_hip = points[11]\n",
    "    right_hip = points[12]\n",
    "    left_knee = points[13]\n",
    "    right_knee = points[14]\n",
    "    left_ankle = points[15]\n",
    "    right_ankle = points[16]\n",
    "    neck = (left_shoulder + right_shoulder) / 2\n",
    "\n",
    "    body_parts = {\n",
    "        'nose': nose,\n",
    "        'left_shoulder': left_shoulder,\n",
    "        'right_shoulder': right_shoulder,\n",
    "        'left_elbow': left_elbow,\n",
    "        'right_elbow': right_elbow,\n",
    "        'left_wrist': left_wrist,\n",
    "        'right_wrist': right_wrist,\n",
    "        'left_hip': left_hip,\n",
    "        'right_hip': right_hip,\n",
    "        'left_knee': left_knee,\n",
    "        'right_knee': right_knee,\n",
    "        'left_ankle': left_ankle,\n",
    "        'right_ankle': right_ankle,\n",
    "        'neck': neck\n",
    "    }\n",
    "    return body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeaa5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_angle(v1, v2):\n",
    "    numer = np.dot(v1, v2)\n",
    "    denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    if denom == 0:\n",
    "        if np.array_equal(v1, v2):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 180.0\n",
    "    degrees = np.degrees(np.arccos(numer / denom))\n",
    "    return degrees\n",
    "\n",
    "def calculate_angle(point1,point2,point3):\n",
    "    v1 = point1[:2] - point2[:2]\n",
    "    v2 = point3[:2] - point2[:2]\n",
    "    return cosine_angle(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9bb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get angles from key points. \n",
    "def get_angles(keypoints):\n",
    "    n_n_rs = calculate_angle(keypoints['nose'], keypoints['neck'], keypoints['right_elbow'])\n",
    "    n_n_ls = calculate_angle(keypoints['nose'], keypoints['neck'], keypoints['left_elbow'])\n",
    "    n_rs_re = calculate_angle(keypoints['neck'], keypoints['right_shoulder'], keypoints['right_elbow'])\n",
    "    n_ls_le = calculate_angle(keypoints['neck'], keypoints['left_shoulder'], keypoints['left_elbow'])\n",
    "    rs_re_rw = calculate_angle(keypoints['right_shoulder'], keypoints['right_elbow'], keypoints['right_wrist'])\n",
    "    ls_le_lw = calculate_angle(keypoints['left_shoulder'], keypoints['left_elbow'], keypoints['left_wrist'])\n",
    "    n_rh_rk = calculate_angle(keypoints['neck'], keypoints['right_hip'], keypoints['right_knee'])\n",
    "    n_lh_lk = calculate_angle(keypoints['neck'], keypoints['left_hip'], keypoints['left_knee'])\n",
    "    rh_rk_ra = calculate_angle(keypoints['right_hip'], keypoints['right_knee'], keypoints['right_ankle'])\n",
    "    lh_lk_la = calculate_angle(keypoints['left_hip'], keypoints['left_knee'], keypoints['left_ankle'])\n",
    "\n",
    "    angles = {\n",
    "        \"Nose-Neck-Right Shoulder\": n_n_rs,\n",
    "        \"Nose-Neck-Left Shoulder\": n_n_ls,\n",
    "        \"Neck-Right Shoulder-Right Elbow\": n_rs_re,\n",
    "        \"Neck-Left Shoulder-Left Elbow\": n_ls_le,\n",
    "        \"Right Shoulder-Right Elbow-Right Wrist\": rs_re_rw,\n",
    "        \"Left Shoulder-Left Elbow-Left Wrist\": ls_le_lw,\n",
    "        \"Neck-Right Hip-Right Knee\": n_rh_rk,\n",
    "        \"Neck-Left Hip-Left Knee\": n_lh_lk,\n",
    "        \"Right Hip-Right Knee-Right Ankle\": rh_rk_ra,\n",
    "        \"Left Hip-Left Knee-Left Ankle\": lh_lk_la\n",
    "    }\n",
    "\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd500dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(image):\n",
    "    image_tensor = tf.image.resize_with_pad(image, 192, 192)\n",
    "    image_tensor = tf.cast(image_tensor, dtype=tf.int32)\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "    \n",
    "    # Run the model.\n",
    "    outputs = movenet(image_tensor)\n",
    "    keypoints = outputs['output_0'].numpy()[0, 0, :, :]\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b22a4c-a5d3-41d9-a567-b57eebad337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l1_norm(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        raise ValueError(\"Lists must be of the same length.\")\n",
    "    return np.sum(np.abs(np.array(l1) - np.array(l2)))\n",
    "\n",
    "def get_angles_from_model(frame):\n",
    "    # Convert the frame from BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert the frame to a TensorFlow tensor\n",
    "    image = tf.convert_to_tensor(frame_rgb, dtype=tf.int32)\n",
    "    \n",
    "    image_tensor = tf.image.resize_with_pad(image, 192, 192)\n",
    "    image_tensor = tf.cast(image_tensor, dtype=tf.int32)\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "\n",
    "    # Run the model.\n",
    "    outputs = movenet(image_tensor)\n",
    "    keypoints = get_keypoints(outputs)\n",
    "    angles = get_angles(keypoints)\n",
    "    return angles,keypoints\n",
    "\n",
    "  \n",
    "def process_frame(frame, frame_id_from_trainer,mapping_frames):\n",
    "    angles1,keypoints = get_angles_from_model(frame)\n",
    "    angles_only = [float(angles1[key]) for key in angles1]\n",
    "    if(frame_id_from_trainer == len(key_frames_angles)):\n",
    "        return 0\n",
    "    d_cur = l1_norm(key_frames_angles[frame_id_from_trainer], angles_only)\n",
    "    d_next= l1_norm(key_frames_angles[(frame_id_from_trainer+1)%trainer_frame_count], angles_only)\n",
    "    if d_next<d_cur:\n",
    "        frame_id_from_trainer=  (frame_id_from_trainer+1)%trainer_frame_count\n",
    "\n",
    "    mapping_frames.append({'user':keypoints,'trainer':key_frames_points[frame_id_from_trainer]})\n",
    "    return frame_id_from_trainer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00d4c8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67bb817-db72-496c-82eb-d94673d81cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n",
      "0 9\n",
      "0 10\n",
      "0 11\n",
      "0 12\n",
      "0 13\n",
      "0 14\n",
      "0 15\n",
      "0 16\n",
      "0 17\n",
      "0 18\n",
      "0 19\n",
      "0 20\n",
      "0 21\n",
      "0 22\n",
      "0 23\n",
      "0 24\n",
      "0 25\n",
      "0 26\n",
      "0 27\n",
      "0 28\n",
      "0 29\n",
      "0 30\n",
      "0 31\n",
      "0 32\n",
      "0 33\n",
      "0 34\n",
      "0 35\n",
      "0 36\n",
      "0 37\n",
      "0 38\n",
      "0 39\n",
      "0 40\n",
      "0 41\n",
      "1 42\n",
      "2 43\n",
      "3 44\n",
      "4 45\n",
      "5 46\n",
      "6 47\n",
      "7 48\n",
      "8 49\n",
      "9 50\n",
      "9 51\n",
      "9 52\n",
      "9 53\n",
      "9 54\n",
      "9 55\n",
      "9 56\n",
      "9 57\n",
      "9 58\n",
      "10 59\n",
      "10 60\n",
      "10 61\n",
      "11 62\n",
      "11 63\n",
      "11 64\n",
      "11 65\n",
      "12 66\n",
      "12 67\n",
      "12 68\n",
      "12 69\n",
      "12 70\n",
      "12 71\n",
      "12 72\n",
      "12 73\n",
      "12 74\n",
      "12 75\n",
      "12 76\n",
      "12 77\n",
      "12 78\n",
      "12 79\n",
      "12 80\n",
      "12 81\n",
      "12 82\n",
      "12 83\n",
      "12 84\n",
      "12 85\n",
      "12 86\n",
      "12 87\n",
      "12 88\n",
      "12 89\n",
      "12 90\n",
      "12 91\n",
      "12 92\n",
      "12 93\n",
      "12 94\n",
      "12 95\n",
      "12 96\n",
      "13 97\n",
      "14 98\n",
      "15 99\n",
      "16 100\n",
      "17 101\n",
      "17 102\n",
      "17 103\n",
      "17 104\n",
      "17 105\n",
      "17 106\n",
      "17 107\n",
      "17 108\n",
      "17 109\n",
      "17 110\n",
      "17 111\n",
      "17 112\n",
      "17 113\n",
      "17 114\n",
      "17 115\n",
      "17 116\n",
      "17 117\n",
      "17 118\n",
      "17 119\n",
      "17 120\n",
      "17 121\n",
      "17 122\n",
      "17 123\n",
      "17 124\n",
      "17 125\n",
      "17 126\n",
      "17 127\n",
      "17 128\n",
      "17 129\n",
      "17 130\n",
      "17 131\n",
      "17 132\n",
      "17 133\n",
      "17 134\n",
      "17 135\n",
      "17 136\n",
      "17 137\n",
      "17 138\n",
      "17 139\n",
      "17 140\n",
      "17 141\n",
      "17 142\n",
      "17 143\n",
      "17 144\n",
      "17 145\n",
      "17 146\n",
      "17 147\n",
      "17 148\n",
      "17 149\n",
      "17 150\n",
      "17 151\n",
      "17 152\n",
      "17 153\n",
      "17 154\n",
      "17 155\n",
      "17 156\n",
      "17 157\n",
      "17 158\n",
      "17 159\n",
      "17 160\n",
      "17 161\n",
      "17 162\n",
      "17 163\n",
      "17 164\n",
      "17 165\n",
      "17 166\n",
      "17 167\n",
      "17 168\n",
      "17 169\n",
      "17 170\n",
      "17 171\n",
      "17 172\n",
      "17 173\n",
      "17 174\n",
      "17 175\n",
      "17 176\n",
      "17 177\n",
      "17 178\n",
      "17 179\n",
      "17 180\n",
      "17 181\n",
      "17 182\n",
      "0 183\n",
      "0 184\n",
      "0 185\n",
      "1 186\n",
      "2 187\n",
      "3 188\n",
      "4 189\n",
      "5 190\n",
      "6 191\n",
      "7 192\n",
      "8 193\n",
      "9 194\n",
      "9 195\n",
      "9 196\n",
      "9 197\n",
      "9 198\n",
      "9 199\n",
      "9 200\n",
      "10 201\n",
      "10 202\n",
      "10 203\n",
      "11 204\n",
      "11 205\n",
      "11 206\n",
      "11 207\n",
      "11 208\n",
      "11 209\n",
      "12 210\n",
      "12 211\n",
      "12 212\n",
      "12 213\n",
      "12 214\n",
      "12 215\n",
      "12 216\n",
      "12 217\n",
      "12 218\n",
      "12 219\n",
      "12 220\n",
      "12 221\n",
      "12 222\n",
      "13 223\n",
      "14 224\n",
      "15 225\n",
      "15 226\n",
      "15 227\n",
      "16 228\n",
      "17 229\n",
      "17 230\n",
      "17 231\n",
      "17 232\n",
      "17 233\n",
      "17 234\n",
      "17 235\n",
      "17 236\n",
      "17 237\n",
      "17 238\n",
      "17 239\n",
      "17 240\n",
      "17 241\n",
      "0 242\n",
      "0 243\n",
      "0 244\n",
      "0 245\n",
      "0 246\n",
      "0 247\n",
      "0 248\n",
      "0 249\n",
      "0 250\n",
      "0 251\n",
      "0 252\n",
      "0 253\n",
      "0 254\n",
      "0 255\n",
      "0 256\n",
      "0 257\n",
      "0 258\n",
      "0 259\n",
      "0 260\n",
      "0 261\n",
      "0 262\n",
      "0 263\n",
      "0 264\n",
      "0 265\n",
      "0 266\n",
      "0 267\n",
      "0 268\n",
      "0 269\n",
      "0 270\n",
      "0 271\n",
      "0 272\n",
      "0 273\n",
      "0 274\n",
      "0 275\n",
      "0 276\n",
      "0 277\n",
      "0 278\n",
      "0 279\n",
      "0 280\n",
      "0 281\n",
      "0 282\n",
      "0 283\n",
      "0 284\n",
      "0 285\n",
      "0 286\n",
      "0 287\n",
      "0 288\n",
      "0 289\n"
     ]
    }
   ],
   "source": [
    "mapping_frames = []\n",
    "\n",
    "def main():\n",
    "    video = cv2.VideoCapture(\"wrong_pushup.mp4\")\n",
    "    count = 0\n",
    "    wait_count = 0\n",
    "    frame_id = 0\n",
    "    num_frames = 0\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip 4 out of 5 frames\n",
    "        frame_id = process_frame(frame, frame_id,mapping_frames)\n",
    "\n",
    "        print(frame_id,num_frames )\n",
    "\n",
    "        # if a != -1:\n",
    "        #     frame_id = a\n",
    "        # else:\n",
    "        #     wait_count += 1\n",
    "        \n",
    "        # if wait_count > 15:\n",
    "        #     frame_id += 1\n",
    "\n",
    "        if (frame_id == 0):\n",
    "            count += 1\n",
    "        num_frames += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b64e621b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapping_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "921a9419-d6bd-49e6-8806-a662e641c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([d['trainer'] for d in mapping_frames])\n",
    "Y = np.array([d['user'] for d in mapping_frames])\n",
    "Y = np.array([[frame[key] for key in frame] for frame in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db9027b-8ac7-4994-829e-ebea8920d275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 14, 3)\n",
      "(290, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eab4c73d-d0ae-4f43-8c16-c3618436b630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46218392, 0.8383655 , 0.55697256],\n",
       "       [0.43625045, 0.70156425, 0.7310258 ],\n",
       "       [0.44442147, 0.7325984 , 0.7976516 ],\n",
       "       [0.5691482 , 0.65825725, 0.6244845 ],\n",
       "       [0.589617  , 0.67819065, 0.5829863 ],\n",
       "       [0.6766716 , 0.66571444, 0.6589128 ],\n",
       "       [0.7245861 , 0.7045313 , 0.86488986],\n",
       "       [0.48684642, 0.4871504 , 0.7160763 ],\n",
       "       [0.4907505 , 0.48461443, 0.6765286 ],\n",
       "       [0.5681819 , 0.29134303, 0.5311303 ],\n",
       "       [0.5722824 , 0.29169476, 0.8500042 ],\n",
       "       [0.6168352 , 0.10930014, 0.78519607],\n",
       "       [0.62781   , 0.08928271, 0.7610698 ],\n",
       "       [0.44033596, 0.7170813 , 0.76433873]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229c8fe4-ec91-4d3a-9def-8da9e452369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_2d(frame):\n",
    "    sliced_matrix = frame[:, :, :2]  # This will have the shape (40, 14, 2)\n",
    "    reshaped_matrix = sliced_matrix.reshape(-1, 2)  # Reshape to (40*14, 2)\n",
    "    return reshaped_matrix\n",
    "X1 = convert_to_2d(X)\n",
    "Y1 = convert_to_2d(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff7f9e44-1fa4-4a0c-a162-138253d672b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized A:\n",
      "[[ 0.73802818  0.02137247]\n",
      " [-0.04365653  0.92121191]]\n",
      "Optimized b:\n",
      "[ 0.07700037 -0.07318382]\n"
     ]
    }
   ],
   "source": [
    "# We will show positions to user based on trainer video. For that affine transformation from trainer to user should be done\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def affine_transformation(X, Y):\n",
    "    n, m = X.shape\n",
    "\n",
    "    def residuals(params):\n",
    "        A = params[:m*m].reshape(m, m)\n",
    "        b = params[m*m:]\n",
    "        return (Y - (X @ A + b)).flatten()\n",
    "\n",
    "    initial_guess = np.zeros((m * m) + m)\n",
    "    result = least_squares(residuals, initial_guess)\n",
    "    optimized_params = result.x\n",
    "    A_opt = optimized_params[:m*m].reshape(m, m)\n",
    "    b_opt = optimized_params[m*m:]\n",
    "    \n",
    "    return A_opt, b_opt\n",
    "\n",
    "A_opt, b_opt = affine_transformation(X1, Y1)\n",
    "\n",
    "print(\"Optimized A:\")\n",
    "print(A_opt)\n",
    "print(\"Optimized b:\")\n",
    "print(b_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "525d2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines_user(keypoints,frame,width,height):\n",
    "    connections = [\n",
    "        (0, 1), (1, 2), (2, 3), (0, 4), (4, 5), (5, 6),  # Head\n",
    "        (6, 8), (8, 10), (5, 7), (7, 9),  # Arms\n",
    "        (6, 12), (12, 14), (14, 16), (5, 11), (11, 13), (13, 15),  # Legs\n",
    "        (11, 12)  \n",
    "    ]\n",
    "\n",
    "    for pt in connections:\n",
    "        x,y = pt\n",
    "        if(keypoints[x][2] > 0.3 and keypoints[y][2] > 0.3):\n",
    "            x_a = int(keypoints[x][1]*width)\n",
    "            y_a = int(keypoints[x][0]*height)\n",
    "            x_b = int(keypoints[y][1]*width)\n",
    "            y_b = int(keypoints[y][0]*height)\n",
    "            cv2.line(frame, (x_a, y_a), (x_b, y_b), (0, 0, 255), 2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8922882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lines_trainer(points, frame,width,height):\n",
    "    connections = [\n",
    "        (0, 1), (1,2), (1,3), (2,4), (4,6) , (3,5), (5,7), (1,8), (1,9), (8,10), (9,11), (10,12), (11,13)\n",
    "    ]\n",
    "\n",
    "    for pt in connections:\n",
    "        x,y = pt\n",
    "        x_a = int(points[x][1])\n",
    "        y_a = int(points[x][0])\n",
    "        x_b = int(points[y][1])\n",
    "        y_b = int(points[y][0])\n",
    "        cv2.line(frame, (x_a, y_a), (x_b, y_b), (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d998f224",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "video2 = cv2.VideoCapture('wrong_pushup.mp4')\n",
    "index = 0\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "out = cv2.VideoWriter('output_pushup.mp4', fourcc, 30.0, (int(video2.get(3)), int(video2.get(4))))\n",
    "while True:\n",
    "    ret, frame = video2.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.int32)\n",
    "    \n",
    "    keypoints = get_data(image)\n",
    "    height, width, _ = frame.shape\n",
    "    points = []\n",
    "    for keypoint in X[index]:\n",
    "        y, x, confidence = keypoint\n",
    "        # if confidence > 0.3:\n",
    "        y_new, x_new = np.matmul(np.array([y, x]), A_opt) + b_opt\n",
    "        x_new = int(x_new * width)\n",
    "        y_new = int(y_new * height)\n",
    "        cv2.circle(frame, (x_new, y_new), 5, (0, 255, 0), -1)\n",
    "        points.append([y_new, x_new])\n",
    "    index += 1\n",
    "    # print(index)\n",
    "    points = np.array(points)\n",
    "    draw_lines_user(keypoints,frame,width,height)\n",
    "    draw_lines_trainer(points,frame,width,height)\n",
    "    # print(points.shape)\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video2.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

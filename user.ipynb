{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4914f25b",
   "metadata": {},
   "source": [
    "# User Code \n",
    "\n",
    "This notebook takes user video , processes the key frames to compare them to the trainer frames stored in csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c3177",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d16227-931c-4436-adde-2ffca0348577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 22:50:43.538940: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-09 22:50:43.573479: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-09 22:50:43.798024: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-09 22:50:44.002359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-09 22:50:44.102561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-09 22:50:44.132658: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-09 22:50:44.414309: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-09 22:50:47.762375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import get_angles   \n",
    "import csv\n",
    "\n",
    "pose_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "movenet = pose_model.signatures['serving_default']\n",
    "\n",
    "key_frames_angles = []\n",
    "key_frames_points = []\n",
    "mapping_frames = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31e1e03-c6ce-489b-888c-78cf8260f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "excersise_name = \"Bicep_Curls\"\n",
    "min_l1_norm = 45 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6fe6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2d4a0d-e6db-4a8f-88bc-feb5778878e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(excersise_name,\"key_frame_angles.csv\")\n",
    "\n",
    "with open(csv_file, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = [dict(row) for row in reader]\n",
    "\n",
    "csv_file = os.path.join(excersise_name,\"key_frame_points.csv\")\n",
    "\n",
    "with open(csv_file, \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data2 = [dict(row) for row in reader]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1f2618-f2a8-4126-b252-32b4e3045ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_frames_angles = [[float(d[key]) for key in d] for d in data]\n",
    "key_frames_points = [[float(d[key]) for key in d] for d in data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Returns key points by taking model output on a image\n",
    "def get_keypoints(outputs):\n",
    "    points = outputs['output_0'].numpy()[0, 0, :, :]\n",
    "    nose = points[0]\n",
    "    left_shoulder = points[5]\n",
    "    right_shoulder = points[6]\n",
    "    left_elbow = points[7]\n",
    "    right_elbow = points[8]\n",
    "    left_wrist = points[9]\n",
    "    right_wrist = points[10]\n",
    "    left_hip = points[11]\n",
    "    right_hip = points[12]\n",
    "    left_knee = points[13]\n",
    "    right_knee = points[14]\n",
    "    left_ankle = points[15]\n",
    "    right_ankle = points[16]\n",
    "    neck = (left_shoulder + right_shoulder) / 2\n",
    "\n",
    "    body_parts = {\n",
    "        'nose': nose,\n",
    "        'left_shoulder': left_shoulder,\n",
    "        'right_shoulder': right_shoulder,\n",
    "        'left_elbow': left_elbow,\n",
    "        'right_elbow': right_elbow,\n",
    "        'left_wrist': left_wrist,\n",
    "        'right_wrist': right_wrist,\n",
    "        'left_hip': left_hip,\n",
    "        'right_hip': right_hip,\n",
    "        'left_knee': left_knee,\n",
    "        'right_knee': right_knee,\n",
    "        'left_ankle': left_ankle,\n",
    "        'right_ankle': right_ankle,\n",
    "        'neck': neck\n",
    "    }\n",
    "    return body_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_angle(v1, v2):\n",
    "    numer = np.dot(v1, v2)\n",
    "    denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    if denom == 0:\n",
    "        if np.array_equal(v1, v2):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 180.0\n",
    "    degrees = np.degrees(np.arccos(numer / denom))\n",
    "    return degrees\n",
    "\n",
    "def calculate_angle(point1,point2,point3):\n",
    "    v1 = point1[:2] - point2[:2]\n",
    "    v2 = point3[:2] - point2[:2]\n",
    "    return cosine_angle(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get angles from key points. \n",
    "def get_angles(keypoints):\n",
    "    n_n_rs = calculate_angle(keypoints['nose'], keypoints['neck'], keypoints['right_elbow'])\n",
    "    n_n_ls = calculate_angle(keypoints['nose'], keypoints['neck'], keypoints['left_elbow'])\n",
    "    n_rs_re = calculate_angle(keypoints['neck'], keypoints['right_shoulder'], keypoints['right_elbow'])\n",
    "    n_ls_le = calculate_angle(keypoints['neck'], keypoints['left_shoulder'], keypoints['left_elbow'])\n",
    "    rs_re_rw = calculate_angle(keypoints['right_shoulder'], keypoints['right_elbow'], keypoints['right_wrist'])\n",
    "    ls_le_lw = calculate_angle(keypoints['left_shoulder'], keypoints['left_elbow'], keypoints['left_wrist'])\n",
    "    n_rh_rk = calculate_angle(keypoints['neck'], keypoints['right_hip'], keypoints['right_knee'])\n",
    "    n_lh_lk = calculate_angle(keypoints['neck'], keypoints['left_hip'], keypoints['left_knee'])\n",
    "    rh_rk_ra = calculate_angle(keypoints['right_hip'], keypoints['right_knee'], keypoints['right_ankle'])\n",
    "    lh_lk_la = calculate_angle(keypoints['left_hip'], keypoints['left_knee'], keypoints['left_ankle'])\n",
    "\n",
    "    angles = {\n",
    "        \"Nose-Neck-Right Shoulder\": n_n_rs,\n",
    "        \"Nose-Neck-Left Shoulder\": n_n_ls,\n",
    "        \"Neck-Right Shoulder-Right Elbow\": n_rs_re,\n",
    "        \"Neck-Left Shoulder-Left Elbow\": n_ls_le,\n",
    "        \"Right Shoulder-Right Elbow-Right Wrist\": rs_re_rw,\n",
    "        \"Left Shoulder-Left Elbow-Left Wrist\": ls_le_lw,\n",
    "        \"Neck-Right Hip-Right Knee\": n_rh_rk,\n",
    "        \"Neck-Left Hip-Left Knee\": n_lh_lk,\n",
    "        \"Right Hip-Right Knee-Right Ankle\": rh_rk_ra,\n",
    "        \"Left Hip-Left Knee-Left Ankle\": lh_lk_la\n",
    "    }\n",
    "\n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd500dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(image):\n",
    "    image_tensor = tf.image.resize_with_pad(image, 192, 192)\n",
    "    image_tensor = tf.cast(image_tensor, dtype=tf.int32)\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
    "    \n",
    "    # Run the model.\n",
    "    outputs = movenet(image_tensor)\n",
    "    keypoints = outputs['output_0'].numpy()[0, 0, :, :]\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77b22a4c-a5d3-41d9-a567-b57eebad337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l1_norm(l1, l2):\n",
    "    if len(l1) != len(l2):\n",
    "        raise ValueError(\"Lists must be of the same length.\")\n",
    "    return np.sum(np.abs(np.array(l1) - np.array(l2)))\n",
    "\n",
    "def get_angles_from_model(frame):\n",
    "    # Convert the frame from BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert the frame to a TensorFlow tensor\n",
    "    image = tf.convert_to_tensor(frame_rgb, dtype=tf.uint8)\n",
    "    \n",
    "    # Add batch dimension and resize image\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    image = tf.cast(tf.image.resize_with_pad(image, 256, 256), dtype=tf.int32)\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = movenet(image)\n",
    "    \n",
    "    # Extract keypoints and calculate angles\n",
    "    keypoints = get_keypoints(outputs)\n",
    "    angles = get_angles(keypoints)\n",
    "    \n",
    "    return angles, keypoints\n",
    "\n",
    "  \n",
    "def process_frame(frame, frame_id_from_trainer):\n",
    "    angles1,keypoints = get_angles_from_model(pose_model,frame)\n",
    "    angles_only = [float(angles1[key]) for key in angles1]\n",
    "    # closest_frame = None\n",
    "    match  = -1\n",
    "    if(frame_id_from_trainer == len(key_frames_angles)):\n",
    "        return 0\n",
    "    d = l1_norm(key_frames_angles[frame_id_from_trainer], angles_only)\n",
    "    if d<min_l1_norm:\n",
    "        match = frame_id_from_trainer \n",
    "    else:\n",
    "        match = -2\n",
    "    mapping_frames.append({'user':keypoints,'trainer':key_frames_points[match],'matched': True if match != -2 else False})\n",
    "    ## Show Key points\n",
    "    return match + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a67bb817-db72-496c-82eb-d94673d81cfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    video = cv2.VideoCapture(\"video.mp4\")\n",
    "    count = 0\n",
    "    wait_count = 0\n",
    "    frame_id = 0\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip 4 out of 5 frames\n",
    "        a = process_frame(frame, frame_id)\n",
    "\n",
    "        if a != -1:\n",
    "            frame_id = a\n",
    "        else:\n",
    "            wait_count += 1\n",
    "        \n",
    "        if wait_count > 15:\n",
    "            frame_id += 1\n",
    "\n",
    "        if (frame_id == 0):\n",
    "            count += 1\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff7f9e44-1fa4-4a0c-a162-138253d672b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized A:\n",
      "[[-3.05576183e-01  3.66827517e-01  1.82466468e+00  9.31846962e-01\n",
      "   6.25945053e-01  1.86865320e+00  8.09803612e-01 -2.64995032e+00\n",
      "  -1.95681268e+01  5.15337527e+00]\n",
      " [-5.93160470e-01 -3.12347886e-01 -6.61773886e-01 -4.74806028e-01\n",
      "  -7.11568308e-01 -4.58321494e-01 -4.95456147e-01  6.40778314e-01\n",
      "   3.66814494e+00 -4.91705486e-01]\n",
      " [-5.58187050e-01 -8.89039976e-02  5.47680610e-01  3.04891419e-02\n",
      "  -1.21371152e+00 -1.54019834e+00 -5.25199514e-03 -5.18059545e-01\n",
      "  -5.40668024e+00  1.43527657e+00]\n",
      " [-1.59684317e-01 -2.93652407e-01  2.37214494e-01  5.61081503e-01\n",
      "   3.77990220e-01  1.18700975e+00  1.75370752e-01 -3.98110132e-01\n",
      "  -3.19989236e+00  2.20245792e+00]\n",
      " [ 2.08582404e-01 -1.02604093e-02 -2.08026831e-01 -1.54688494e-01\n",
      "   4.67150903e-01  2.15167647e-01 -6.80840357e-02  4.90616888e-01\n",
      "   2.90582486e+00 -6.91321021e-01]\n",
      " [-8.51197076e-02 -1.14830530e-02  2.87683604e-02  9.45849087e-02\n",
      "   3.13454447e-01  4.47960065e-01  3.55133570e-02 -2.77218538e-01\n",
      "  -1.05475423e+00  3.39661617e-01]\n",
      " [ 2.63546899e-01  1.71569927e-01 -1.18944583e-01 -4.51906369e-01\n",
      "   5.20834958e-01  6.69257561e-02 -2.40774073e-01  1.29023232e+00\n",
      "   8.55323061e+00 -5.41885644e+00]\n",
      " [ 1.17736250e-01 -2.26195137e-02 -6.51633333e-02  7.32809301e-02\n",
      "   4.73340049e-01  3.45715893e-01  4.97620420e-02 -3.30479808e-02\n",
      "  -6.31664890e-01  4.78712247e-01]\n",
      " [-4.85077274e-02 -9.31732670e-03 -3.69985902e-02 -1.09687031e-01\n",
      "  -3.69344371e-01 -3.31207702e-01 -7.05554410e-02  1.53447906e-01\n",
      "   1.58027791e+00 -1.69462540e-01]\n",
      " [ 6.01384700e-02 -2.23385242e-02  8.41229910e-02  6.54808669e-02\n",
      "   3.40847912e-01  3.61624849e-01  2.39163588e-02 -2.31536080e-02\n",
      "  -6.57262733e-01  1.02414720e+00]]\n",
      "Optimized b:\n",
      "[256.33417078 207.58591852   1.12371542 102.84144148  17.11837258\n",
      " -89.29629667 205.98507119 155.41162127 636.86759002 144.42956213]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([d['trainer'] for d in mapping_frames])\n",
    "Y = np.array([d['user'] for d in mapping_frames])\n",
    "\n",
    "# We will show positions to user based on trainer video. For that affine transformation from trainer to user should be done\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def affine_transformation(X, Y):\n",
    "    n, m = X.shape\n",
    "\n",
    "    def residuals(params):\n",
    "        A = params[:m*m].reshape(m, m)\n",
    "        b = params[m*m:]\n",
    "        return (Y - (X @ A + b)).flatten()\n",
    "\n",
    "    initial_guess = np.zeros((m * m) + m)\n",
    "    result = least_squares(residuals, initial_guess)\n",
    "    optimized_params = result.x\n",
    "    A_opt = optimized_params[:m*m].reshape(m, m)\n",
    "    b_opt = optimized_params[m*m:]\n",
    "    \n",
    "    return A_opt, b_opt\n",
    "\n",
    "A_opt, b_opt = affine_transformation(X, Y)\n",
    "\n",
    "print(\"Optimized A:\")\n",
    "print(A_opt)\n",
    "print(\"Optimized b:\")\n",
    "print(b_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "video2 = cv2.VideoCapture('wrong_pushup.mp4')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "fps = int(video2.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(video2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_writer = cv2.VideoWriter('wrong_pushup.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# for d in mapping_frames:\n",
    "#     y, x, confidence = keypoint\n",
    "#     if confidence > 0.3:\n",
    "#         x = int(x * frame_width)\n",
    "#         y = int(y * frame_height)\n",
    "#         cv2.circle(frame1, (x, y), 5, (0, 0, 255), -1)\n",
    "\n",
    "image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "image = tf.convert_to_tensor(image,dtype=tf.int32)\n",
    "keypoints = get_data(image)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
